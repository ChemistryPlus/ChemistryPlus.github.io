{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55bbd39e-1a5b-479a-bef9-f73fe8651f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dataclasses\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import sklearn\n",
    "import torch\n",
    "import safetensors\n",
    "import IPython.display as ipy_display\n",
    "sns.set_style('darkgrid')\n",
    "import subprocess\n",
    "import shutil                # ← to delete old mmseqs2 files/folders\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sys import platform\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "from xgboost import XGBRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.ensemble   import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost            import XGBClassifier\n",
    "from ngboost            import NGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f2eeb-7985-4807-b6ae-5aa0fb91c459",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ec7ca5e-d57d-47c0-9b07-4120e3753a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Sequence\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float, Integer\n",
    "from enum import StrEnum, auto\n",
    "\n",
    "StringArray = Sequence[str]\n",
    "Array = Tensor | np.ndarray\n",
    "ArrayInt = Integer[Array, 'n']\n",
    "DiscreteArray = ArrayInt | StringArray\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SplitTuple:\n",
    "    train : Sequence[int]\n",
    "    val: Sequence[int]\n",
    "    test: Sequence[int]\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Raises error if split parts overlap.\"\"\"\n",
    "        for a, b in [('train', 'val'), ('val', 'test'), ('test', 'train')]:\n",
    "            overlap = np.intersect1d(getattr(self, a), getattr(self, b))\n",
    "            if overlap.shape[0]:\n",
    "                raise ValueError(\n",
    "                    f'Found {len(overlap)} overlapping items between {a} and {b}.'\n",
    "                )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.train) + len(self.val) + len(self.test)\n",
    "\n",
    "    def trainval(self) -> DiscreteArray:\n",
    "        return np.concatenate([self.train, self.val], axis=0)\n",
    "\n",
    "    def indices(self) -> DiscreteArray:\n",
    "        return np.concatenate([self.train, self.val, self.test], axis=0)\n",
    "\n",
    "    def as_dict(self) -> dict[str, DiscreteArray]:\n",
    "        return dataclasses.asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_lists(\n",
    "        cls,\n",
    "        train: DiscreteArray,\n",
    "        val: DiscreteArray,\n",
    "        test: DiscreteArray\n",
    "    ) -> 'SplitTuple':\n",
    "        return cls(\n",
    "            train=np.asarray(train),\n",
    "            val=np.asarray(val),\n",
    "            test=np.asarray(test)\n",
    "        )\n",
    "        \n",
    "def load_embeddings(file):\n",
    "    tensors = {}\n",
    "    with safetensors.safe_open(str(file), framework='pt') as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k).numpy()\n",
    "    return tensors\n",
    "    \n",
    "class ArrayMap:\n",
    "    \"\"\"Map discrete values (sequences, int, smiles) to arrays.\"\"\"\n",
    "\n",
    "    def __init__(self, keys, values):\n",
    "\n",
    "        self.mapper = pd.Series(data=np.arange(len(keys)), index=keys)\n",
    "        self.values = values\n",
    "        # Check for duplicates.\n",
    "        dups = self.mapper.index[self.mapper.index.duplicated()].tolist()\n",
    "        if dups:\n",
    "            raise ValueError(f'Found {len(dups)} duplicated isosmiles={dups}')\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, adict) -> ArrayMap:\n",
    "        return ArrayMap(np.array(list(adict.keys())), np.stack(list(adict.values())))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fname: str) -> ArrayMap:\n",
    "        data = load_embeddings(fname)\n",
    "        return cls.from_dict(data)\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.mapper.index)\n",
    "        \n",
    "\n",
    "    def __call__(self, key_array: types.DiscreteArray):\n",
    "        assert isinstance(key_array, np.ndarray), 'expected np.ndarray as input!'\n",
    "        return self.values[self.mapper[key_array].values]\n",
    "        \n",
    "def html_header(text:str, n:int = 2)-> None:\n",
    "    ipy_display.display(ipy_display.HTML(f'<h{n}>{text}</h{n}>'))\n",
    "\n",
    "    \n",
    "def peek_df(df:pd.DataFrame, title:str=None, n=5)-> None:\n",
    "    if title:\n",
    "        if is_notebook():\n",
    "            html_header(f'{title:-^30}')\n",
    "        else:\n",
    "            print(f'{title:-^30}')\n",
    "        \n",
    "    print(f'Columns: {df.columns}')\n",
    "    print(f'Shape: {df.shape}')\n",
    "    if n > 0 and is_notebook():\n",
    "        ipy_display.display(df.head(n=n))\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    pearson_r, _ = scipy.stats.pearsonr(y_true, y_pred)\n",
    "    r_squared = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "    kendall_tau, _ = scipy.stats.kendalltau(y_true, y_pred)\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    return {'pearson_r': pearson_r, 'R^2': r_squared, 'kendall_tau': kendall_tau, 'mae': mae}\n",
    "\n",
    "def divide_per_cluster(\n",
    "    df_complete: pd.DataFrame,\n",
    "    col_prot: str,\n",
    "    col_sequence: str,\n",
    "    # Optional directory output\n",
    "    directory_out: Path = None, \n",
    "    tresh_identity: float = 0.8,\n",
    "    n_splits: int = 10,\n",
    "    frac_val: float = 0.1,\n",
    "    frac_test: float = 0.1,\n",
    "    seed: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Divides a dataframe based on protein clusters using MMseqs2.\n",
    "    \"\"\"\n",
    "    print(\"Initiating splittting of data based on protein clusters...\")\n",
    "\n",
    "    # --- Part A: Clustering with MMseqs2 ---\n",
    "    df_unique = df_complete[[col_prot, col_sequence]].drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_unique)} sequences for clustering.\")\n",
    "    \n",
    "    base_temp_path = directory_out if directory_out is not None else Path(\"./\")\n",
    "    dir_mmseqs = base_temp_path / \"mmseqs_temp\"\n",
    "    dir_mmseqs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fasta_path = dir_mmseqs / \"sequences.fasta\"\n",
    "    db_path = dir_mmseqs / \"DB\"\n",
    "    cluster_path = dir_mmseqs / \"ClusterDB\"\n",
    "    results_tsv_path = dir_mmseqs / \"clusters.tsv\"\n",
    "\n",
    "    with open(fasta_path, \"w\") as f:\n",
    "        for _, row in df_unique.iterrows():\n",
    "            f.write(f\">{row[col_prot]}\\n{row[col_sequence]}\\n\")\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\"mmseqs\", \"createdb\", str(fasta_path), str(db_path)], check=True, capture_output=True, text=True)\n",
    "        subprocess.run([\n",
    "            \"mmseqs\", \"cluster\", str(db_path), str(cluster_path), str(dir_mmseqs / \"tmp\"),\n",
    "            \"--min-seq-id\", str(tresh_identity)\n",
    "        ], check=True, capture_output=True, text=True)\n",
    "        subprocess.run(\n",
    "            [\"mmseqs\", \"createtsv\", str(db_path), str(db_path), str(cluster_path), str(results_tsv_path)],\n",
    "            check=True, capture_output=True, text=True\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        raise SystemExit(\"ERROR: Couldn't find MMseqs2. Please make sure it is installed and available in your PATH.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise SystemExit(f\"ERROR: MMseqs2 Failed.\\nOutput error:\\n{e.stderr}\")\n",
    "\n",
    "    df_clusters = pd.read_csv(results_tsv_path, sep=\"\\t\", header=None, names=['cluster_id', col_prot])\n",
    "    map_cluster = df_clusters.set_index(col_prot)['cluster_id'].to_dict()\n",
    "    df_complete['cluster_id'] = df_complete[col_prot].map(map_cluster)\n",
    "    \n",
    "    print(f\"Clustering done. Identified {df_complete['cluster_id'].nunique()} clusters.\")\n",
    "\n",
    "    # Initialize dict\n",
    "    split_dict_generado = {}\n",
    "    ids_de_clusters = df_complete['cluster_id'].unique()\n",
    "    \n",
    "    # Splits loop\n",
    "    for i in range(n_splits):\n",
    "        current_seed = seed + i # A seed for each split\n",
    "        \n",
    "        clusters_ent_val, clusters_test = train_test_split(\n",
    "            ids_de_clusters, test_size=frac_test, random_state=current_seed\n",
    "        )\n",
    "        frac_val_ajustada = frac_val / (1 - frac_test) if (1 - frac_test) > 0 else 0\n",
    "        clusters_train, clusters_val = train_test_split(\n",
    "            clusters_ent_val, test_size=frac_val_ajustada, random_state=current_seed\n",
    "        )\n",
    "\n",
    "        # Obtain original datframa indexes\n",
    "        indices_train = df_complete[df_complete['cluster_id'].isin(clusters_train)].index.to_numpy()\n",
    "        indices_val = df_complete[df_complete['cluster_id'].isin(clusters_val)].index.to_numpy()\n",
    "        indices_test = df_complete[df_complete['cluster_id'].isin(clusters_test)].index.to_numpy()\n",
    "\n",
    "        # Creating split tuple instance and saving it\n",
    "        split_key = f'Cluster_{i+1}'\n",
    "        split_dict_generado[split_key] = SplitTuple(\n",
    "            train=indices_train,\n",
    "            val=indices_val,\n",
    "            test=indices_test\n",
    "        )\n",
    "        print(f\"  - Generated {split_key}: Train={len(indices_train)}, Val={len(indices_val)}, Test={len(indices_test)}\")\n",
    "\n",
    "    # --- Part C (Optional): Save and clean ---\n",
    "    if directory_out:\n",
    "        print(\"\\nSaving splits to CSV files...\")\n",
    "        # Saving a full file with \"split\" column for the first split\n",
    "        df_complete['split'] = 'sin_asignar'\n",
    "        split_0 = list(split_dict_generado.values())[0] # First split as an example\n",
    "        df_complete.loc[split_0.train, 'split'] = 'train'\n",
    "        df_complete.loc[split_0.val, 'split'] = 'val'\n",
    "        df_complete.loc[split_0.test, 'split'] = 'test'\n",
    "        \n",
    "        df_final = df_complete.drop(columns=['cluster_id', col_sequence])\n",
    "        nombre_base = f\"dataset_split_id{tresh_identity}\"\n",
    "        df_final.to_csv(directory_out / f\"{nombre_base}_complete_example.csv\", index=False)\n",
    "        print(f\"Saved example split file to {directory_out}\")\n",
    "    \n",
    "    # Clean temporary MMseqs2 files\n",
    "    temp_dir_for_mmseqs = directory_out / \"mmseqs_temp\" if directory_out else Path(\"./mmseqs_temp\")\n",
    "    if temp_dir_for_mmseqs.exists():\n",
    "        shutil.rmtree(temp_dir_for_mmseqs)\n",
    "    \n",
    "    # Return dict\n",
    "    print(\"\\nDONE! Returning split dictionary.\")\n",
    "    return split_dict_generado\n",
    "\n",
    "def generate_mls_splits(\n",
    "    df_complete: pd.DataFrame,\n",
    "    col_cluster: str,\n",
    "    col_y: str,\n",
    "    n_bins_y: int = 5,\n",
    "    n_splits: int = 10,\n",
    "    frac_test: float = 0.2,\n",
    "    frac_val: float = 0.1,\n",
    "    seed: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generates splits using Multilabel Stratification on cluster IDs and the target variable y,\n",
    "    with precise control over test and validation fractions.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_splits} splits (Test: {frac_test:.0%}, Val: {frac_val:.0%})...\")\n",
    "\n",
    "    # --- 1. Prepare labels for stratification (no changes here) ---\n",
    "    # One-Hot-Encode cluster IDs\n",
    "    cluster_ids = pd.factorize(df_complete[col_cluster])[0]\n",
    "    n_clusters = len(np.unique(cluster_ids))\n",
    "    cluster_one_hot = np.zeros((len(df_complete), n_clusters), dtype=bool)\n",
    "    cluster_one_hot[np.arange(len(df_complete)), cluster_ids] = 1\n",
    "\n",
    "    # Bin the continuous target variable 'y' into categories\n",
    "    y_binned = pd.qcut(df_complete[col_y], q=n_bins_y, labels=False, duplicates='drop')\n",
    "    y_one_hot = pd.get_dummies(y_binned).to_numpy(dtype=bool)\n",
    "\n",
    "    # Create the \"Super-Label\" by combining cluster and y labels\n",
    "    split_labels = np.hstack([cluster_one_hot, y_one_hot])\n",
    "    print(f\"Created stratification matrix with shape {split_labels.shape} (rows, clusters + y_bins)\")\n",
    "    \n",
    "    # --- 2. Corrected Splitting Logic ---\n",
    "    split_dict_generated = {}\n",
    "    indices = np.arange(len(df_complete))\n",
    "    \n",
    "    # Main loop to create n_splits different splits\n",
    "    for i in range(n_splits):\n",
    "        current_seed = seed + i\n",
    "        \n",
    "        # --- STEP A: First split (trainval / test) ---\n",
    "        # We use MultilabelStratifiedShuffleSplit to gain control over `frac_test`.\n",
    "        first_splitter = MultilabelStratifiedShuffleSplit(\n",
    "            n_splits=1, # We only want one split from this\n",
    "            test_size=frac_test, \n",
    "            random_state=current_seed\n",
    "        )\n",
    "        # Get the first (and only) split's indices\n",
    "        trainval_idx, test_idx = next(first_splitter.split(indices, split_labels))\n",
    "\n",
    "        # --- STEP B: Second split (train / val) from the trainval set ---\n",
    "        relative_val_size = frac_val / (1.0 - frac_test) if (1.0 - frac_test) > 0 else 0\n",
    "        \n",
    "        second_splitter = MultilabelStratifiedShuffleSplit(\n",
    "            n_splits=1,\n",
    "            test_size=relative_val_size,\n",
    "            random_state=current_seed\n",
    "        )\n",
    "        train_idx_rel, val_idx_rel = next(second_splitter.split(trainval_idx, split_labels[trainval_idx]))\n",
    "\n",
    "        train_idx = trainval_idx[train_idx_rel]\n",
    "        val_idx = trainval_idx[val_idx_rel]\n",
    "        \n",
    "        split_key = f'Stratified_{i+1}'\n",
    "        split_dict_generated[split_key] = SplitTuple(\n",
    "            train=train_idx,\n",
    "            val=val_idx,\n",
    "            test=test_idx\n",
    "        )\n",
    "        print(f\"  - Generated {split_key}: Train={len(train_idx)}, Val={len(val_idx)}, Test={len(test_idx)}\")\n",
    "        \n",
    "    print(\"\\nDONE! Returning Multilabel Stratified split dictionary.\")\n",
    "    return split_dict_generated\n",
    "\n",
    "def loco_balanced(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    col_cluster: str,                 # p.ej. 'embedding_cluster_id'\n",
    "    col_y: str | None = None,         # continuo o binario (solo si ensure_both_classes=True)\n",
    "    n_splits: int = 0,                # <-- ignorados (compatibilidad)\n",
    "    frac_test: float = 0.0,           # <-- ignorados\n",
    "    frac_val: float = 0.0,            # <-- ignorados\n",
    "    random_state: int = 0,            # <-- semilla solo si añades aleatoriedad\n",
    "    n_bins_y: int = 5,                # <-- ignorado\n",
    "    # ---- parámetros de evaluación por clúster ----\n",
    "    target_clusters: list[int] | None = None,  # p.ej. [4, 5]\n",
    "    col_protein: str | None = None,            # columna con el nombre de proteína\n",
    "    max_pairs_per_cluster: int | None = None,  # None = todas las parejas\n",
    "    ensure_both_classes: bool = False,         # exigir ambas clases en test/val\n",
    "    threshold: float = -1.5,                   # si col_y es continuo\n",
    "    emit_baseline_variants: bool = False       # variantes _BASE (val vacío; train+=val)\n",
    ") -> dict[str, SplitTuple]:\n",
    "    \"\"\"\n",
    "    SOLO modo intra-clúster: para cada clúster en target_clusters,\n",
    "    empareja proteínas y crea splits (test=protA, val=protB) y el inverso.\n",
    "    \"\"\"\n",
    "    assert target_clusters is not None, \"Debes pasar target_clusters=[...]\"\n",
    "    assert col_protein is not None, \"Debes pasar col_protein=nombre_columna_de_proteína\"\n",
    "\n",
    "    all_idx = df.index.to_numpy()\n",
    "    splits: dict[str, SplitTuple] = {}\n",
    "\n",
    "    # Preparar etiqueta binaria si se exige doble clase\n",
    "    if ensure_both_classes:\n",
    "        assert col_y is not None, \"col_y requerido si ensure_both_classes=True\"\n",
    "        y_series = df[col_y]\n",
    "        if y_series.dropna().nunique() == 2:\n",
    "            ybin = (y_series.astype(int) > 0).to_numpy()\n",
    "        else:\n",
    "            ybin = (y_series.to_numpy() > threshold)\n",
    "\n",
    "        def both_classes(ix: np.ndarray) -> bool:\n",
    "            if ix.size == 0: \n",
    "                return False\n",
    "            u = np.unique(ybin[ix])\n",
    "            return u.size == 2\n",
    "\n",
    "    for cid in target_clusters:\n",
    "        # proteínas únicas dentro del clúster\n",
    "        prots = (\n",
    "            df.loc[df[col_cluster] == cid, col_protein]\n",
    "              .dropna().unique().tolist()\n",
    "        )\n",
    "        if len(prots) < 2:\n",
    "            continue\n",
    "\n",
    "        pairs = list(combinations(prots, 2))\n",
    "        if max_pairs_per_cluster is not None:\n",
    "            pairs = pairs[:max_pairs_per_cluster]\n",
    "\n",
    "        for pa, pb in pairs:\n",
    "            # dos órdenes: (A test, B val) y (B test, A val)\n",
    "            for order_id, (test_p, val_p) in enumerate([(pa, pb), (pb, pa)], start=1):\n",
    "                test_idx = df.index[(df[col_cluster] == cid) & (df[col_protein] == test_p)].to_numpy()\n",
    "                val_idx  = df.index[(df[col_cluster] == cid) & (df[col_protein] == val_p)].to_numpy()\n",
    "\n",
    "                # si se exige doble clase y falla, intenta swap; si sigue fallando, omite\n",
    "                if ensure_both_classes:\n",
    "                    if not (both_classes(test_idx) and both_classes(val_idx)):\n",
    "                        test_idx, val_idx = val_idx, test_idx\n",
    "                    if not (both_classes(test_idx) and both_classes(val_idx)):\n",
    "                        # omite este split\n",
    "                        continue\n",
    "\n",
    "                train_idx = np.setdiff1d(all_idx, np.concatenate([test_idx, val_idx]), assume_unique=False)\n",
    "\n",
    "                key = f\"LOCO_intraC{cid}_{test_p}_TEST_{val_p}_VAL_{order_id}\"\n",
    "                splits[key] = SplitTuple(train=train_idx, val=val_idx, test=test_idx)\n",
    "\n",
    "                if emit_baseline_variants:\n",
    "                    base_key = key + \"_BASE\"\n",
    "                    train_plus_val = np.unique(np.concatenate([train_idx, val_idx]))\n",
    "                    splits[base_key] = SplitTuple(\n",
    "                        train=train_plus_val,\n",
    "                        val=np.empty(0, dtype=train_idx.dtype),  # val vacío (para baselines)\n",
    "                        test=test_idx\n",
    "                    )\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7cf14-6067-4cf7-b572-b6933c74021e",
   "metadata": {},
   "source": [
    "# Paths and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e47756c8-a71a-4759-ba38-6cf0d518ae45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PathsAndConstants at 0x7a27618f4090>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PathsAndConstants:\n",
    "    data_path = Path('../data/processed')\n",
    "    label_csv = data_path / 'protein_substrate.csv'\n",
    "    protein_sequences_csv = data_path / 'proteins.csv' \n",
    "    results_path = Path('../results')\n",
    "    protein = 'protein_name'\n",
    "    sequences_csv = data_path / 'proteins.csv'\n",
    "    T33 = 'ESM2_T33'\n",
    "    T6 = 'ESM2_T6'\n",
    "    trilo = 'ESMC'\n",
    "    protein_embs = {T6: data_path / 'protein_esm2t6.safetensors', T33: data_path / 'protein_esm2t33.safetensors', trilo: data_path / 'protein_esmc_600m.safetensors'}\n",
    "    mol = 'substrate_name'\n",
    "    mol_embs = {'RDKit': data_path / 'substrate_rdkit2d3d.safetensors'}\n",
    "    raw_label = 'value'\n",
    "    measurement = 'Specific Activity'\n",
    "    label = f'log10({measurement})'\n",
    "    unit='μmol.min⁻¹.mg⁻¹'\n",
    "\n",
    "_C = PathsAndConstants()\n",
    "_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af81541-d811-4689-9f67-5f1253fd21a5",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f0a2f23-09f6-480b-a490-ae5c62aed7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['protein_name', 'substrate_name', 'substrate notes', 'measurement',\n",
      "       'units', 'value', 'variance', 'log10(Specific Activity)',\n",
      "       'binary_label'],\n",
      "      dtype='object')\n",
      "(385, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_name</th>\n",
       "      <th>substrate_name</th>\n",
       "      <th>substrate notes</th>\n",
       "      <th>measurement</th>\n",
       "      <th>units</th>\n",
       "      <th>value</th>\n",
       "      <th>variance</th>\n",
       "      <th>log10(Specific Activity)</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExeGalOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.315970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MreGalOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.758155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PfeGalOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>182.00</td>\n",
       "      <td>11.60</td>\n",
       "      <td>2.260071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FoxGalOxB</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>60.70</td>\n",
       "      <td>5.77</td>\n",
       "      <td>1.783189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UmaRafOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.769551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PhuRafOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.619789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AflAlcOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>65.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.812913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PruAlcOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>79.90</td>\n",
       "      <td>29.40</td>\n",
       "      <td>1.902547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FoxAlcOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.080922</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AsyAlcOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.481486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PorAlcOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.232996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CglAlcOx</td>\n",
       "      <td>D-Galactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.086186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ExeGalOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>6.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.797268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MreGalOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.285557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PfeGalOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>69.70</td>\n",
       "      <td>11.20</td>\n",
       "      <td>1.843233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FoxGalOxB</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>16.20</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.209515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UmaRafOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.397940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PhuRafOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.096910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AflAlcOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.849419</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PruAlcOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>12.20</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.086360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FoxAlcOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.958607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AsyAlcOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.602060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PorAlcOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.064458</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CglAlcOx</td>\n",
       "      <td>D-Lactose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.397940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ExeGalOx</td>\n",
       "      <td>Raffinose</td>\n",
       "      <td>300mM</td>\n",
       "      <td>Specific Activity</td>\n",
       "      <td>μmol.min⁻¹.mg⁻¹</td>\n",
       "      <td>9.06</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.957128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   protein_name substrate_name substrate notes        measurement  \\\n",
       "0      ExeGalOx    D-Galactose           300mM  Specific Activity   \n",
       "1      MreGalOx    D-Galactose           300mM  Specific Activity   \n",
       "2      PfeGalOx    D-Galactose           300mM  Specific Activity   \n",
       "3     FoxGalOxB    D-Galactose           300mM  Specific Activity   \n",
       "4      UmaRafOx    D-Galactose           300mM  Specific Activity   \n",
       "5      PhuRafOx    D-Galactose           300mM  Specific Activity   \n",
       "6      AflAlcOx    D-Galactose           300mM  Specific Activity   \n",
       "7      PruAlcOx    D-Galactose           300mM  Specific Activity   \n",
       "8      FoxAlcOx    D-Galactose           300mM  Specific Activity   \n",
       "9      AsyAlcOx    D-Galactose           300mM  Specific Activity   \n",
       "10     PorAlcOx    D-Galactose           300mM  Specific Activity   \n",
       "11     CglAlcOx    D-Galactose           300mM  Specific Activity   \n",
       "12     ExeGalOx      D-Lactose           300mM  Specific Activity   \n",
       "13     MreGalOx      D-Lactose           300mM  Specific Activity   \n",
       "14     PfeGalOx      D-Lactose           300mM  Specific Activity   \n",
       "15    FoxGalOxB      D-Lactose           300mM  Specific Activity   \n",
       "16     UmaRafOx      D-Lactose           300mM  Specific Activity   \n",
       "17     PhuRafOx      D-Lactose           300mM  Specific Activity   \n",
       "18     AflAlcOx      D-Lactose           300mM  Specific Activity   \n",
       "19     PruAlcOx      D-Lactose           300mM  Specific Activity   \n",
       "20     FoxAlcOx      D-Lactose           300mM  Specific Activity   \n",
       "21     AsyAlcOx      D-Lactose           300mM  Specific Activity   \n",
       "22     PorAlcOx      D-Lactose           300mM  Specific Activity   \n",
       "23     CglAlcOx      D-Lactose           300mM  Specific Activity   \n",
       "24     ExeGalOx      Raffinose           300mM  Specific Activity   \n",
       "\n",
       "              units   value  variance  log10(Specific Activity)  binary_label  \n",
       "0   μmol.min⁻¹.mg⁻¹   20.70      0.42                  1.315970             1  \n",
       "1   μmol.min⁻¹.mg⁻¹    5.73      1.72                  0.758155             1  \n",
       "2   μmol.min⁻¹.mg⁻¹  182.00     11.60                  2.260071             1  \n",
       "3   μmol.min⁻¹.mg⁻¹   60.70      5.77                  1.783189             1  \n",
       "4   μmol.min⁻¹.mg⁻¹    0.17      0.01                 -0.769551             1  \n",
       "5   μmol.min⁻¹.mg⁻¹    0.24      0.02                 -0.619789             1  \n",
       "6   μmol.min⁻¹.mg⁻¹   65.00      1.41                  1.812913             1  \n",
       "7   μmol.min⁻¹.mg⁻¹   79.90     29.40                  1.902547             1  \n",
       "8   μmol.min⁻¹.mg⁻¹    0.83      0.22                 -0.080922             1  \n",
       "9   μmol.min⁻¹.mg⁻¹    0.33      0.11                 -0.481486             1  \n",
       "10  μmol.min⁻¹.mg⁻¹    1.71      0.89                  0.232996             1  \n",
       "11  μmol.min⁻¹.mg⁻¹    0.82      0.04                 -0.086186             1  \n",
       "12  μmol.min⁻¹.mg⁻¹    6.27      0.45                  0.797268             1  \n",
       "13  μmol.min⁻¹.mg⁻¹    1.93      0.20                  0.285557             1  \n",
       "14  μmol.min⁻¹.mg⁻¹   69.70     11.20                  1.843233             1  \n",
       "15  μmol.min⁻¹.mg⁻¹   16.20      1.16                  1.209515             1  \n",
       "16  μmol.min⁻¹.mg⁻¹    0.04      0.01                 -1.397940             1  \n",
       "17  μmol.min⁻¹.mg⁻¹    0.08      0.02                 -1.096910             1  \n",
       "18  μmol.min⁻¹.mg⁻¹    7.07      0.37                  0.849419             1  \n",
       "19  μmol.min⁻¹.mg⁻¹   12.20      2.12                  1.086360             1  \n",
       "20  μmol.min⁻¹.mg⁻¹    0.11      0.01                 -0.958607             1  \n",
       "21  μmol.min⁻¹.mg⁻¹    0.25      0.12                 -0.602060             1  \n",
       "22  μmol.min⁻¹.mg⁻¹    1.16      0.90                  0.064458             1  \n",
       "23  μmol.min⁻¹.mg⁻¹    0.40      0.02                 -0.397940             1  \n",
       "24  μmol.min⁻¹.mg⁻¹    9.06      1.45                  0.957128             1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(_C.label_csv)\n",
    "df[_C.label]= np.log10(df[_C.raw_label])\n",
    "proteins = df[_C.protein].to_numpy(str)\n",
    "mols     = df[_C.mol].to_numpy(str)\n",
    "y_cont   = df[_C.label].to_numpy(np.float32)\n",
    "# Threshold\n",
    "threshold = -1.5\n",
    "y_class   = (y_cont > threshold).astype(int) # 0 = “zero/low\", 1 = “for regression”\n",
    "df[\"binary_label\"] = y_class\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cc41691-0a41-42e8-8894-49d3d0f24ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_embs = {_C.T6: ArrayMap.load(_C.protein_embs['ESM2_T6']),\n",
    "                _C.T33: ArrayMap.load(_C.protein_embs['ESM2_T33']),\n",
    "                _C.trilo: ArrayMap.load(_C.protein_embs['ESMC'])}\n",
    "mol_embs = {name: ArrayMap.load(file) for name, file in _C.mol_embs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4830266-991e-4d6f-902e-ef2f8d74b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 204)\n",
      "(2, 325)\n",
      "(2, 1285)\n",
      "(2, 1157)\n"
     ]
    }
   ],
   "source": [
    "print(mol_embs['RDKit'](np.array(['D-Galactose', 'D-Galactose'])).shape)\n",
    "print(protein_embs[_C.T6](np.array(['FoxGalOxB', 'UmaRafOx'])).shape)\n",
    "print(protein_embs[_C.T33](np.array(['FoxGalOxB', 'UmaRafOx'])).shape)\n",
    "print(protein_embs[_C.trilo](np.array(['FoxGalOxB', 'UmaRafOx'])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb422012",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_seq = pd.read_csv(_C.sequences_csv)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"ERROR\")\n",
    "protein_sequences_map = df_seq.set_index('protein_name')['sequence'].to_dict()\n",
    "col_seqs = 'protein_sequence'\n",
    "df[col_seqs] = df[_C.protein].map(protein_sequences_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722597a",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f91cf2-2aff-403c-986e-6738549e6ac1",
   "metadata": {},
   "source": [
    "## Sub 2: Classifier splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e4ada",
   "metadata": {},
   "source": [
    "### Sub 3: Random and MMSeqs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe6d8a45-78c9-4c9e-bf5b-e4c43366bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = {}\n",
    "trainval_size = 0.8\n",
    "indices = np.array(df.index)\n",
    "n_splits = 10\n",
    "\n",
    "for i in range(n_splits):\n",
    "    trainval_idx, test_idx = train_test_split(\n",
    "        indices,\n",
    "        train_size=trainval_size,\n",
    "        shuffle=True,\n",
    "        random_state=i\n",
    "    )\n",
    "    split_dict[f'Random_{i+1}'] = SplitTuple(train=trainval_idx, val=np.array([], dtype=int), test=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aefdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating splittting of data based on protein clusters...\n",
      "Found 12 sequences for clustering.\n",
      "Clustering done. Identified 11 clusters.\n",
      "  - Generated Cluster_1: Train=258, Val=66, Test=61\n",
      "  - Generated Cluster_2: Train=218, Val=102, Test=65\n",
      "  - Generated Cluster_3: Train=273, Val=51, Test=61\n",
      "  - Generated Cluster_4: Train=247, Val=65, Test=73\n",
      "  - Generated Cluster_5: Train=225, Val=93, Test=67\n",
      "  - Generated Cluster_6: Train=256, Val=68, Test=61\n",
      "  - Generated Cluster_7: Train=270, Val=54, Test=61\n",
      "  - Generated Cluster_8: Train=254, Val=66, Test=65\n",
      "  - Generated Cluster_9: Train=234, Val=88, Test=63\n",
      "  - Generated Cluster_10: Train=254, Val=58, Test=73\n",
      "\n",
      "Saving splits to CSV files...\n",
      "Saved example split file to ../results/data_splits\n",
      "\n",
      "DONE! Returning split dictionary.\n"
     ]
    }
   ],
   "source": [
    "#MMSeqs2 clustering and splits:\n",
    "split_dict = divide_per_cluster(\n",
    "    df_complete=df,\n",
    "    col_prot=_C.protein,\n",
    "    col_sequence=col_seqs,\n",
    "    directory_out=_C.results_path / \"data_splits\",\n",
    "    tresh_identity=0.6,\n",
    "    n_splits=10,\n",
    "    frac_val=0.15,\n",
    "    frac_test=0.15,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# veryfing splits\n",
    "first_split = list(split_dict.values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65918d",
   "metadata": {},
   "source": [
    "### Sub 3: Multilabel Stratified Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7820a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix: (12, 1157)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "emb_clustering = protein_embs[_C.trilo] #change the protein embeddings HERE\n",
    "unique_protein_names = df[_C.protein].unique()\n",
    "protein_embedding_matrix = emb_clustering(unique_protein_names)\n",
    "\n",
    "# --- Distance matrix --\n",
    "print(f\"Distance matrix: {protein_embedding_matrix.shape}\")\n",
    "\n",
    "# --- Dendogram ---\n",
    "linked_matrix = linkage(protein_embedding_matrix, method='average', metric='cosine')\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(\n",
    "    linked_matrix,\n",
    "    orientation='top',\n",
    "    labels=unique_protein_names,\n",
    "    distance_sort='descending',\n",
    "    show_leaf_counts=True\n",
    ")\n",
    "plt.title('Dendrogram of Proteins using ESMC as embeddings')\n",
    "plt.ylabel('Cosine distance')\n",
    "plt.xlabel('Proteins')\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74ef2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 6 clusters using 0.8:\n",
      "Clúster 0: ['PfeGalOx', 'AflAlcOx', 'PruAlcOx', 'AsyAlcOx']\n",
      "Clúster 1: ['FoxGalOxB']\n",
      "Clúster 2: ['ExeGalOx', 'MreGalOx']\n",
      "Clúster 3: ['FoxAlcOx']\n",
      "Clúster 4: ['UmaRafOx', 'PhuRafOx']\n",
      "Clúster 5: ['PorAlcOx', 'CglAlcOx']\n",
      "\n",
      "Column 'embedding_cluster_id' added.\n",
      "embedding_cluster_id\n",
      "0    132\n",
      "1     35\n",
      "2     51\n",
      "3     34\n",
      "4     66\n",
      "5     67\n",
      "Name: n_rows, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Distance threshold ---\n",
    "DISTANCE_THRESHOLD = 0.8\n",
    "protein_cluster_labels = fcluster(linked_matrix, t=DISTANCE_THRESHOLD, criterion='distance')\n",
    "protein_cluster_labels = protein_cluster_labels - 1\n",
    "\n",
    "map_protein_to_cluster = dict(zip(unique_protein_names, protein_cluster_labels))\n",
    "n_found_clusters = len(np.unique(protein_cluster_labels))\n",
    "\n",
    "print(f\"\\nFound {n_found_clusters} clusters using {DISTANCE_THRESHOLD}:\")\n",
    "for cluster_id in sorted(np.unique(protein_cluster_labels)):\n",
    "    members = [name for name, cid in map_protein_to_cluster.items() if cid == cluster_id]\n",
    "    print(f\"Clúster {cluster_id}: {members}\")\n",
    "\n",
    "df['embedding_cluster_id'] = df[_C.protein].map(map_protein_to_cluster)\n",
    "print(\"\\nColumn 'embedding_cluster_id' added.\")\n",
    "\n",
    "cluster_sizes_rows = (\n",
    "    df['embedding_cluster_id']\n",
    "      .value_counts(dropna=False)  # pon dropna=True si quieres ignorar filas no mapeadas\n",
    "      .sort_index()\n",
    "      .rename('n_rows')\n",
    ")\n",
    "print(cluster_sizes_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88e10f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 splits (Test: 10%, Val: 10%)...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label_4class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_4class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m generate_mls_splits(\n\u001b[1;32m      2\u001b[0m     df_complete\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m      3\u001b[0m     col_cluster\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_cluster_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     col_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_4class\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     n_bins_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m      6\u001b[0m     n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      7\u001b[0m     frac_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     frac_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m¡Splits dict with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(split_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m divisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplits keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(split_dict\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 248\u001b[0m, in \u001b[0;36mgenerate_mls_splits\u001b[0;34m(df_complete, col_cluster, col_y, n_bins_y, n_splits, frac_test, frac_val, seed)\u001b[0m\n\u001b[1;32m    245\u001b[0m cluster_one_hot[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(df_complete)), cluster_ids] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Bin the continuous target variable 'y' into categories\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m y_binned \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mqcut(df_complete[col_y], q\u001b[38;5;241m=\u001b[39mn_bins_y, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, duplicates\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    249\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(y_binned)\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Create the \"Super-Label\" by combining cluster and y labels\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_4class'"
     ]
    }
   ],
   "source": [
    "split_dict = generate_mls_splits(\n",
    "    df_complete=df,\n",
    "    col_cluster='embedding_cluster_id',\n",
    "    col_y='label_4class',\n",
    "    n_bins_y=6,\n",
    "    n_splits=10,\n",
    "    frac_test=0.1,\n",
    "    frac_val=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n¡Splits dict with {len(split_dict)} divisions.\")\n",
    "print(f\"Splits keys: {list(split_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9fa9c",
   "metadata": {},
   "source": [
    "### Sub 3: LOCO Balance Splitting (NEEDS REWORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb069cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1 319 32 34\n",
      "LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 319 34 32\n"
     ]
    }
   ],
   "source": [
    "splits = loco_balanced(\n",
    "    df,\n",
    "    col_cluster='embedding_cluster_id',\n",
    "    col_y='binary_label',\n",
    "    target_clusters=[4, 5],\n",
    "    col_protein=_C.protein,\n",
    "    ensure_both_classes=True,      # evita test/val con una sola clase\n",
    "    threshold=-1.5,\n",
    "    emit_baseline_variants=False   # puedes dejarlo en False si no quieres *_BASE\n",
    ")\n",
    "for k, s in splits.items():\n",
    "    print(k, len(s.train), len(s.val), len(s.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35da0ca",
   "metadata": {},
   "source": [
    "## Sub 2: Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b804c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503129d472134c58b74a3ca57e627e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:49:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=0.4702 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2090 val_loss=0.0000 scale=2.0000 norm=2.6302\n",
      "[iter 200] loss=0.1789 val_loss=0.0000 scale=1.0000 norm=1.2705\n",
      "[iter 300] loss=0.1714 val_loss=0.0000 scale=0.1250 norm=0.1574\n",
      "[iter 400] loss=0.1702 val_loss=0.0000 scale=0.0156 norm=0.0197\n",
      "[iter 0] loss=0.4702 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1920 val_loss=0.0000 scale=2.0000 norm=2.5898\n",
      "[iter 200] loss=0.1643 val_loss=0.0000 scale=1.0000 norm=1.2594\n",
      "[iter 300] loss=0.1558 val_loss=0.0000 scale=1.0000 norm=1.2508\n",
      "[iter 400] loss=0.1513 val_loss=0.0000 scale=0.2500 norm=0.3116\n",
      "[iter 0] loss=0.4702 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1922 val_loss=0.0000 scale=2.0000 norm=2.5819\n",
      "[iter 200] loss=0.1633 val_loss=0.0000 scale=1.0000 norm=1.2562\n",
      "[iter 300] loss=0.1553 val_loss=0.0000 scale=1.0000 norm=1.2492\n",
      "[iter 400] loss=0.1513 val_loss=0.0000 scale=0.2500 norm=0.3116\n",
      "[iter 0] loss=0.4567 val_loss=0.0000 scale=4.0000 norm=8.0000\n",
      "[iter 100] loss=0.2032 val_loss=0.0000 scale=2.0000 norm=2.6079\n",
      "[iter 200] loss=0.1663 val_loss=0.0000 scale=1.0000 norm=1.2521\n",
      "[iter 300] loss=0.1524 val_loss=0.0000 scale=1.0000 norm=1.2322\n",
      "[iter 400] loss=0.1452 val_loss=0.0000 scale=1.0000 norm=1.2231\n",
      "[iter 0] loss=0.4567 val_loss=0.0000 scale=4.0000 norm=8.0000\n",
      "[iter 100] loss=0.1853 val_loss=0.0000 scale=2.0000 norm=2.5859\n",
      "[iter 200] loss=0.1596 val_loss=0.0000 scale=1.0000 norm=1.2618\n",
      "[iter 300] loss=0.1518 val_loss=0.0000 scale=0.2500 norm=0.3132\n",
      "[iter 400] loss=0.1492 val_loss=0.0000 scale=0.0625 norm=0.0782\n",
      "[iter 0] loss=0.4567 val_loss=0.0000 scale=4.0000 norm=8.0000\n",
      "[iter 100] loss=0.1848 val_loss=0.0000 scale=2.0000 norm=2.5770\n",
      "[iter 200] loss=0.1551 val_loss=0.0000 scale=1.0000 norm=1.2505\n",
      "[iter 300] loss=0.1483 val_loss=0.0000 scale=0.2500 norm=0.3111\n",
      "[iter 400] loss=0.1461 val_loss=0.0000 scale=0.5000 norm=0.6214\n",
      "[iter 0] loss=0.4521 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1960 val_loss=0.0000 scale=2.0000 norm=2.5793\n",
      "[iter 200] loss=0.1625 val_loss=0.0000 scale=1.0000 norm=1.2396\n",
      "[iter 300] loss=0.1536 val_loss=0.0000 scale=0.5000 norm=0.6135\n",
      "[iter 400] loss=0.1509 val_loss=0.0000 scale=0.2500 norm=0.3059\n",
      "[iter 0] loss=0.4521 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1870 val_loss=0.0000 scale=2.0000 norm=2.5635\n",
      "[iter 200] loss=0.1601 val_loss=0.0000 scale=1.0000 norm=1.2455\n",
      "[iter 300] loss=0.1523 val_loss=0.0000 scale=1.0000 norm=1.2355\n",
      "[iter 400] loss=0.1493 val_loss=0.0000 scale=0.1250 norm=0.1540\n",
      "[iter 0] loss=0.4521 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1870 val_loss=0.0000 scale=2.0000 norm=2.5635\n",
      "[iter 200] loss=0.1601 val_loss=0.0000 scale=1.0000 norm=1.2455\n",
      "[iter 300] loss=0.1523 val_loss=0.0000 scale=1.0000 norm=1.2355\n",
      "[iter 400] loss=0.1493 val_loss=0.0000 scale=0.1250 norm=0.1540\n",
      "[iter 0] loss=0.4474 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1853 val_loss=0.0000 scale=2.0000 norm=2.5553\n",
      "[iter 200] loss=0.1564 val_loss=0.0000 scale=1.0000 norm=1.2376\n",
      "[iter 300] loss=0.1496 val_loss=0.0000 scale=0.5000 norm=0.6145\n",
      "[iter 400] loss=0.1477 val_loss=0.0000 scale=0.1250 norm=0.1533\n",
      "[iter 0] loss=0.4474 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1770 val_loss=0.0000 scale=2.0000 norm=2.5404\n",
      "[iter 200] loss=0.1491 val_loss=0.0000 scale=1.0000 norm=1.2309\n",
      "[iter 300] loss=0.1406 val_loss=0.0000 scale=1.0000 norm=1.2203\n",
      "[iter 400] loss=0.1370 val_loss=0.0000 scale=0.5000 norm=0.6079\n",
      "[iter 0] loss=0.4474 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1788 val_loss=0.0000 scale=2.0000 norm=2.5483\n",
      "[iter 200] loss=0.1488 val_loss=0.0000 scale=1.0000 norm=1.2335\n",
      "[iter 300] loss=0.1403 val_loss=0.0000 scale=0.5000 norm=0.6114\n",
      "[iter 400] loss=0.1362 val_loss=0.0000 scale=0.1250 norm=0.1522\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2008 val_loss=0.0000 scale=2.0000 norm=2.6098\n",
      "[iter 200] loss=0.1673 val_loss=0.0000 scale=1.0000 norm=1.2577\n",
      "[iter 300] loss=0.1582 val_loss=0.0000 scale=1.0000 norm=1.2461\n",
      "[iter 400] loss=0.1554 val_loss=0.0000 scale=0.1250 norm=0.1553\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1858 val_loss=0.0000 scale=2.0000 norm=2.5708\n",
      "[iter 200] loss=0.1553 val_loss=0.0000 scale=1.0000 norm=1.2408\n",
      "[iter 300] loss=0.1474 val_loss=0.0000 scale=0.1250 norm=0.1536\n",
      "[iter 400] loss=0.1463 val_loss=0.0000 scale=0.0312 norm=0.0384\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2014 val_loss=0.0000 scale=2.0000 norm=2.6389\n",
      "[iter 200] loss=0.1645 val_loss=0.0000 scale=1.0000 norm=1.2666\n",
      "[iter 300] loss=0.1535 val_loss=0.0000 scale=2.0000 norm=2.5056\n",
      "[iter 400] loss=0.1469 val_loss=0.0000 scale=2.0000 norm=2.4858\n",
      "[iter 0] loss=0.4567 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1960 val_loss=0.0000 scale=2.0000 norm=2.5988\n",
      "[iter 200] loss=0.1588 val_loss=0.0000 scale=1.0000 norm=1.2458\n",
      "[iter 300] loss=0.1450 val_loss=0.0000 scale=1.0000 norm=1.2251\n",
      "[iter 400] loss=0.1399 val_loss=0.0000 scale=0.5000 norm=0.6087\n",
      "[iter 0] loss=0.4567 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1775 val_loss=0.0000 scale=2.0000 norm=2.5512\n",
      "[iter 200] loss=0.1506 val_loss=0.0000 scale=1.0000 norm=1.2404\n",
      "[iter 300] loss=0.1435 val_loss=0.0000 scale=0.2500 norm=0.3080\n",
      "[iter 400] loss=0.1409 val_loss=0.0000 scale=0.0078 norm=0.0096\n",
      "[iter 0] loss=0.4567 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1773 val_loss=0.0000 scale=4.0000 norm=5.0761\n",
      "[iter 200] loss=0.1409 val_loss=0.0000 scale=1.0000 norm=1.2181\n",
      "[iter 300] loss=0.1308 val_loss=0.0000 scale=1.0000 norm=1.2042\n",
      "[iter 400] loss=0.1266 val_loss=0.0000 scale=1.0000 norm=1.1984\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2007 val_loss=0.0000 scale=2.0000 norm=2.5976\n",
      "[iter 200] loss=0.1631 val_loss=0.0000 scale=1.0000 norm=1.2439\n",
      "[iter 300] loss=0.1473 val_loss=0.0000 scale=0.5000 norm=0.6112\n",
      "[iter 400] loss=0.1394 val_loss=0.0000 scale=0.5000 norm=0.6063\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1778 val_loss=0.0000 scale=2.0000 norm=2.5577\n",
      "[iter 200] loss=0.1469 val_loss=0.0000 scale=1.0000 norm=1.2363\n",
      "[iter 300] loss=0.1370 val_loss=0.0000 scale=0.2500 norm=0.3062\n",
      "[iter 400] loss=0.1332 val_loss=0.0000 scale=0.0625 norm=0.0763\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1883 val_loss=0.0000 scale=2.0000 norm=2.5961\n",
      "[iter 200] loss=0.1542 val_loss=0.0000 scale=1.0000 norm=1.2508\n",
      "[iter 300] loss=0.1448 val_loss=0.0000 scale=0.5000 norm=0.6201\n",
      "[iter 400] loss=0.1419 val_loss=0.0000 scale=0.5000 norm=0.6183\n",
      "[iter 0] loss=0.4710 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2044 val_loss=0.0000 scale=2.0000 norm=2.6206\n",
      "[iter 200] loss=0.1710 val_loss=0.0000 scale=1.0000 norm=1.2611\n",
      "[iter 300] loss=0.1593 val_loss=0.0000 scale=1.0000 norm=1.2449\n",
      "[iter 400] loss=0.1564 val_loss=0.0000 scale=0.5000 norm=0.6212\n",
      "[iter 0] loss=0.4710 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=2.6095\n",
      "[iter 200] loss=0.1705 val_loss=0.0000 scale=1.0000 norm=1.2667\n",
      "[iter 300] loss=0.1618 val_loss=0.0000 scale=1.0000 norm=1.2558\n",
      "[iter 400] loss=0.1583 val_loss=0.0000 scale=0.0001 norm=0.0002\n",
      "[iter 0] loss=0.4710 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1936 val_loss=0.0000 scale=2.0000 norm=2.5969\n",
      "[iter 200] loss=0.1648 val_loss=0.0000 scale=1.0000 norm=1.2615\n",
      "[iter 300] loss=0.1589 val_loss=0.0000 scale=1.0000 norm=1.2562\n",
      "[iter 400] loss=0.1565 val_loss=0.0000 scale=0.5000 norm=0.6270\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2106 val_loss=0.0000 scale=2.0000 norm=2.6330\n",
      "[iter 200] loss=0.1736 val_loss=0.0000 scale=1.0000 norm=1.2623\n",
      "[iter 300] loss=0.1591 val_loss=0.0000 scale=1.0000 norm=1.2431\n",
      "[iter 400] loss=0.1519 val_loss=0.0000 scale=1.0000 norm=1.2338\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.2071 val_loss=0.0000 scale=2.0000 norm=2.6406\n",
      "[iter 200] loss=0.1765 val_loss=0.0000 scale=1.0000 norm=1.2759\n",
      "[iter 300] loss=0.1658 val_loss=0.0000 scale=1.0000 norm=1.2618\n",
      "[iter 400] loss=0.1612 val_loss=0.0000 scale=0.2500 norm=0.3140\n",
      "[iter 0] loss=0.4620 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1972 val_loss=0.0000 scale=2.0000 norm=2.6041\n",
      "[iter 200] loss=0.1647 val_loss=0.0000 scale=1.0000 norm=1.2565\n",
      "[iter 300] loss=0.1558 val_loss=0.0000 scale=0.5000 norm=0.6215\n",
      "[iter 400] loss=0.1513 val_loss=0.0000 scale=0.1250 norm=0.1547\n",
      "[iter 0] loss=0.4613 val_loss=0.0000 scale=4.0000 norm=8.0000\n",
      "[iter 100] loss=0.1978 val_loss=0.0000 scale=2.0000 norm=2.5853\n",
      "[iter 200] loss=0.1658 val_loss=0.0000 scale=1.0000 norm=1.2472\n",
      "[iter 300] loss=0.1529 val_loss=0.0000 scale=1.0000 norm=1.2294\n",
      "[iter 400] loss=0.1460 val_loss=0.0000 scale=0.5000 norm=0.6102\n",
      "[iter 0] loss=0.4613 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1943 val_loss=0.0000 scale=2.0000 norm=2.6015\n",
      "[iter 200] loss=0.1605 val_loss=0.0000 scale=1.0000 norm=1.2559\n",
      "[iter 300] loss=0.1512 val_loss=0.0000 scale=0.5000 norm=0.6228\n",
      "[iter 400] loss=0.1485 val_loss=0.0000 scale=0.2500 norm=0.3107\n",
      "[iter 0] loss=0.4613 val_loss=0.0000 scale=2.0000 norm=4.0000\n",
      "[iter 100] loss=0.1935 val_loss=0.0000 scale=2.0000 norm=2.5953\n",
      "[iter 200] loss=0.1600 val_loss=0.0000 scale=1.0000 norm=1.2537\n",
      "[iter 300] loss=0.1510 val_loss=0.0000 scale=1.0000 norm=1.2437\n",
      "[iter 400] loss=0.1488 val_loss=0.0000 scale=1.0000 norm=1.2415\n"
     ]
    }
   ],
   "source": [
    "# ——— Classifiers —————————————————————————\n",
    "model_makers = {\n",
    "    'Logistic': lambda: LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "    'RF':       lambda: RandomForestClassifier(\n",
    "        n_estimators=100, random_state=42\n",
    "    ),\n",
    "    'XGBoost':  lambda: XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'NGBoost':  lambda: NGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# ——— Results and predictions loop ————————\n",
    "results = []\n",
    "predict_dict = {}\n",
    "\n",
    "combined_iterator = itertools.product(\n",
    "    model_makers.items(),\n",
    "    split_dict.items(),\n",
    "    protein_embs.items(),\n",
    "    mol_embs.items()\n",
    ")\n",
    "total_iterations = (\n",
    "    len(model_makers) *\n",
    "    len(split_dict) *\n",
    "    len(protein_embs) *\n",
    "    len(mol_embs)\n",
    ")\n",
    "\n",
    "for (model_name, model_fn), (split_name, split), \\\n",
    "    (p_name, p_emb), (m_name, m_emb) in tqdm(\n",
    "        combined_iterator,\n",
    "        total=total_iterations\n",
    "):\n",
    "    # Keys for predict_dict\n",
    "    result_key = (model_name, split_name, p_name, m_name)\n",
    "\n",
    "    # Info\n",
    "    info = {\n",
    "        'model':             model_name,\n",
    "        'split':             split_name,\n",
    "        'protein_embeddings':p_name,\n",
    "        'substrate_embeddings':m_name\n",
    "    }\n",
    "\n",
    "    # Splits index\n",
    "    tr_idx = split.trainval()\n",
    "    ts_idx = split.test\n",
    "\n",
    "    # input building\n",
    "    x_tr = np.concatenate([\n",
    "        p_emb(proteins[tr_idx]),\n",
    "        m_emb(mols[tr_idx])\n",
    "    ], axis=-1)\n",
    "    x_ts = np.concatenate([\n",
    "        p_emb(proteins[ts_idx]),\n",
    "        m_emb(mols[ts_idx])\n",
    "    ], axis=-1)\n",
    "\n",
    "    y_tr = y_class[tr_idx]\n",
    "    y_ts = y_class[ts_idx]\n",
    "\n",
    "    # train and prediction\n",
    "    clf     = model_fn()\n",
    "    clf.fit(x_tr, y_tr)\n",
    "    y_pred  = clf.predict(x_ts)\n",
    "    y_proba = clf.predict_proba(x_ts)[:,1]\n",
    "\n",
    "    # --- SAving metrics in`info` ---\n",
    "    info |= {\n",
    "        'auprc' : average_precision_score(y_ts, y_proba),\n",
    "        'f1_score':  f1_score(y_ts, y_pred, zero_division=0),\n",
    "        'roc_auc':   roc_auc_score(y_ts, y_proba)\n",
    "    }\n",
    "\n",
    "    # --- Saving predictions to expand in rows ---\n",
    "    predict_dict[result_key] = {\n",
    "        \"y_pred\":  y_pred,\n",
    "        \"y_proba\": y_proba,\n",
    "        \"y_true\":  y_ts,\n",
    "        \"proteins\": proteins[ts_idx],\n",
    "        \"mols\":     mols[ts_idx]\n",
    "    }\n",
    "\n",
    "    results.append(info)\n",
    "\n",
    "# ——— 4. Saving in rows —————\n",
    "rows = []\n",
    "for (model_name, split_name, p_name, m_name), pred in predict_dict.items():\n",
    "    for prot, mol, yt, yp, yp_p in zip(\n",
    "        pred[\"proteins\"],\n",
    "        pred[\"mols\"],\n",
    "        pred[\"y_true\"],\n",
    "        pred[\"y_pred\"],\n",
    "        pred[\"y_proba\"]\n",
    "    ):\n",
    "        rows.append({\n",
    "            \"model\":           model_name,\n",
    "            \"split\":           split_name,\n",
    "            \"protein_name\":    prot,\n",
    "            \"substrate_name\":  mol,\n",
    "            \"y_true\":          int(yt),\n",
    "            \"y_pred\":          int(yp),\n",
    "            \"y_proba\":         float(yp_p)\n",
    "        })\n",
    "\n",
    "# ——— 5. DataFrame Building ————————————————\n",
    "class_preds_df  = pd.DataFrame(rows)\n",
    "class_preds_df.to_csv(_C.results_path / 'pred_results' / 'preds_class_data.csv', index=False)\n",
    "class_results_df = pd.DataFrame(results)\n",
    "class_results_df.to_csv(_C.results_path / 'pred_results' / 'class_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e463eb",
   "metadata": {},
   "source": [
    "### Sub 3: Using XGBoost to separate near zero activity from actual activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b620f734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix: (385, 1361)\n",
      "Class distrbution (0: No activity, 1: Activity): \n",
      "1    327\n",
      "0     58\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f68e903ad64a49a397da053979d802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Folds:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/josechemistry/miniforge3/envs/myenv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [07:34:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            fold     auroc\n",
      "0   Stratified_1  0.864865\n",
      "1   Stratified_2  0.838384\n",
      "2   Stratified_3  0.904040\n",
      "3   Stratified_4  0.790179\n",
      "4   Stratified_5  0.951389\n",
      "5   Stratified_6  0.897321\n",
      "6   Stratified_7  0.931429\n",
      "7   Stratified_8  0.945946\n",
      "8   Stratified_9  0.965714\n",
      "9  Stratified_10  0.964706\n",
      "         auroc\n",
      "mean  0.905397\n",
      "std   0.058754\n"
     ]
    }
   ],
   "source": [
    "proteins = df[_C.protein].to_numpy(str)\n",
    "mols     = df[_C.mol].to_numpy(str)\n",
    "y_cont   = df[_C.label].to_numpy(np.float32)\n",
    "\n",
    "threshold = -1.6\n",
    "y_class   = (y_cont > threshold).astype(int)\n",
    "\n",
    "p_emb = protein_embs['ESMC']\n",
    "m_emb = mol_embs['RDKit']\n",
    "x = np.concatenate([p_emb(proteins), m_emb(mols)], axis=-1)\n",
    "\n",
    "print(f\"Shape of matrix: {x.shape}\")\n",
    "print(f\"Class distrbution (0: No activity, 1: Activity): \\n{pd.Series(y_class).value_counts()}\")\n",
    "\n",
    "all_results = []\n",
    "out_of_fold_preds = np.zeros(len(df))\n",
    "for split_name, split in tqdm(split_dict.items(), desc=\"Processing Folds\"):\n",
    "    train_idx = split.trainval()\n",
    "    test_idx  = split.test\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y_class[train_idx], y_class[test_idx]\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred_proba = clf.predict_proba(x_test)[:, 1]\n",
    "    y_pred_class = clf.predict(x_test)\n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    all_results.append({\n",
    "        'fold': split_name,\n",
    "        'auroc': auroc\n",
    "    })\n",
    "    out_of_fold_preds[test_idx] = y_pred_proba\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df)\n",
    "summary = results_df.agg({\n",
    "    'auroc': ['mean', 'std']\n",
    "})\n",
    "print(summary)\n",
    "df['xgb_proba'] = out_of_fold_preds\n",
    "df['xgb_pred']  = (out_of_fold_preds > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01990d-96a2-42d0-9bba-6fcddde934be",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eaf980",
   "metadata": {},
   "source": [
    "## Regression splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original: 385 rows.\n",
      "Dataset curated (only positive): 295 rows.\n",
      "Generating 10 splits (Test: 20%, Val: 10%)...\n",
      "Created stratification matrix with shape (295, 9) (rows, clusters + y_bins)\n",
      "  - Generated Stratified_1: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_2: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_3: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_4: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_5: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_6: Train=207, Val=30, Test=58\n",
      "  - Generated Stratified_7: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_8: Train=208, Val=29, Test=58\n",
      "  - Generated Stratified_9: Train=206, Val=29, Test=60\n",
      "  - Generated Stratified_10: Train=205, Val=30, Test=60\n",
      "\n",
      "DONE! Returning Multilabel Stratified split dictionary.\n"
     ]
    }
   ],
   "source": [
    "df_positive = df[df['xgb_pred'] == 1].copy()\n",
    "print(f\"Original Dataset: {len(df)} rows.\")\n",
    "print(f\"Dataset curated (only positive): {len(df_positive)} rows.\")\n",
    "\n",
    "split_reg = generate_mls_splits(\n",
    "    df_complete=df_positive,\n",
    "    col_cluster='embedding_cluster_id',\n",
    "    col_y=_C.label,\n",
    "    n_bins_y=5,\n",
    "    n_splits=10,\n",
    "    frac_test=0.1,\n",
    "    frac_val=0.1,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "988c2d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCO_split1 181 22 22\n",
      "LOCO_split2 181 22 22\n",
      "LOCO_split3 181 22 22\n",
      "LOCO_split4 181 22 22\n",
      "LOCO_split5 181 22 22\n",
      "LOCO_split6 181 22 22\n",
      "LOCO_split7 181 22 22\n",
      "LOCO_split8 181 22 22\n",
      "LOCO_split9 181 22 22\n",
      "LOCO_split10 181 22 22\n"
     ]
    }
   ],
   "source": [
    "df_positive = df[df['xgb_pred'] == 1].copy()\n",
    "splits_regs = loco_balanced(\n",
    "    df             = df_positive,\n",
    "    col_cluster    = 'embedding_cluster_id',\n",
    "    col_y          = None,\n",
    "    n_splits       = 10,\n",
    "    frac_test      = 0.10,\n",
    "    frac_val       = 0.10,\n",
    "    random_state   = 42,\n",
    "    n_bins_y       = 5\n",
    ")\n",
    "\n",
    "for name, s in splits_class.items():\n",
    "    print(name, len(s.train), len(s.val), len(s.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4e126",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "201db8e5-b4f1-4bba-bfe4-03bd7e801cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c773c1ca77444785275fc3d95f9477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'SplitTuple' object has no attribute 'trainval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m result_key \u001b[38;5;241m=\u001b[39m (model_name, split_name, p_name, m_name)\n\u001b[1;32m     33\u001b[0m info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:model_name,\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m:split_name,\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:p_name,\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubstrate_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:m_name}\n\u001b[0;32m---> 37\u001b[0m train_indexes \u001b[38;5;241m=\u001b[39m split\u001b[38;5;241m.\u001b[39mtrainval()\n\u001b[1;32m     38\u001b[0m test_indexes \u001b[38;5;241m=\u001b[39m split\u001b[38;5;241m.\u001b[39mtest\n\u001b[1;32m     39\u001b[0m x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([p_emb(proteins[train_indexes]), m_emb(mols[train_indexes])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SplitTuple' object has no attribute 'trainval'"
     ]
    }
   ],
   "source": [
    "model_makers = {\n",
    "    'RF': RandomForestRegressor,\n",
    "    'Linear': LinearRegression,\n",
    "    'XGBoost': lambda: XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42),\n",
    "    'NGBoost': lambda: NGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "proteins = df[_C.protein].to_numpy(str)\n",
    "mols = df[_C.mol].to_numpy(str)\n",
    "y = df[_C.label].to_numpy(np.float32)\n",
    "\n",
    "total_iterations = (\n",
    "    len(model_makers) *\n",
    "    len(splits_regs) *\n",
    "    len(protein_embs) *\n",
    "    len(mol_embs)\n",
    ")\n",
    "\n",
    "combined_iterator = itertools.product(\n",
    "    model_makers.items(),\n",
    "    splits_regs.items(),\n",
    "    protein_embs.items(),\n",
    "    mol_embs.items()\n",
    ")\n",
    "    \n",
    "results = []\n",
    "predict_dict = {}\n",
    "for (model_name, model_fn), (split_name, split), (p_name, p_emb), (m_name, m_emb) in tqdm(\n",
    "    combined_iterator,\n",
    "    total=total_iterations):\n",
    "    result_key = (model_name, split_name, p_name, m_name)\n",
    "    info = {'model':model_name,\n",
    "            'split':split_name,\n",
    "            'protein_embeddings':p_name,\n",
    "            'substrate_embeddings':m_name}\n",
    "    train_indexes = split.trainval()\n",
    "    test_indexes = split.test\n",
    "    x_train = np.concatenate([p_emb(proteins[train_indexes]), m_emb(mols[train_indexes])], axis=-1)\n",
    "    x_test = np.concatenate([p_emb(proteins[test_indexes]), m_emb(mols[test_indexes])], axis=-1)\n",
    "    y_train = y[train_indexes]\n",
    "    y_test = y[test_indexes]\n",
    "    model = model_fn()\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    info |= evaluate(y_test, y_pred)\n",
    "    predict_dict[result_key] = {\"y_pred\": y_pred, \"y_test\": y_test, \"proteins\": proteins[split.test], \"mols\": mols[split.test]}\n",
    "    results.append(info)\n",
    "\n",
    "rows = []\n",
    "for (model_name, split_name, p_name, m_name), pred in predict_dict.items():\n",
    "    proteins_split = pred[\"proteins\"]\n",
    "    mols_split     = pred[\"mols\"]\n",
    "    y_true_split   = pred[\"y_test\"]\n",
    "    y_pred_split   = pred[\"y_pred\"]\n",
    "    for prot, mol, yt, yp in zip(proteins_split, mols_split, y_true_split, y_pred_split):\n",
    "        rows.append({\n",
    "            \"model\":          model_name,\n",
    "            \"split\":          split_name,\n",
    "            \"protein_name\":   prot,\n",
    "            \"substrate_name\": mol,\n",
    "            \"y_true\":         float(yt),\n",
    "            \"y_pred\":         float(yp)\n",
    "        })\n",
    "\n",
    "splits_df = pd.DataFrame(rows)\n",
    "splits_df.to_csv(_C.results_path / 'pred_results' / 'splits_baselines_data.csv', index=False)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(_C.results_path/'pred_results' / 'baseline_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ccd6d",
   "metadata": {},
   "source": [
    "# Enzymatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ddcc173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskEnzymatch(nn.Module):\n",
    "    def __init__(self, dim_prot, dim_mol, hidden_dim, shared_dim, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.proj_p = nn.Sequential(\n",
    "            nn.Linear(dim_prot, hidden_dim), nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_dim, shared_dim)\n",
    "        )\n",
    "        self.proj_m = nn.Sequential(\n",
    "            nn.Linear(dim_mol, hidden_dim), nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_dim, shared_dim)\n",
    "        )\n",
    "        self.reg_head   = nn.Linear(1, 1)\n",
    "        self.class_head = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x_p, x_m):\n",
    "        zp  = F.normalize(self.proj_p(x_p), dim=1)\n",
    "        zm  = F.normalize(self.proj_m(x_m), dim=1)\n",
    "        sims = (zp * zm).sum(dim=1, keepdim=True)\n",
    "        return sims.squeeze(1)  # will be fed into heads externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0386b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch metrics\n",
    "# Reg\n",
    "def mse_torch(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r2_torch(y_true, y_pred):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def mae_torch(y_true, y_pred):\n",
    "    return nn.L1Loss()(y_pred, y_true)\n",
    "\n",
    "def kendall_tau(y_true, y_pred):\n",
    "    n = y_true.size(0)\n",
    "    if n < 2:\n",
    "        return torch.tensor(0.0, device=y_true.device)\n",
    "    true_diff = y_true.unsqueeze(1) - y_true.unsqueeze(0)\n",
    "    pred_diff = y_pred.unsqueeze(1) - y_pred.unsqueeze(0)\n",
    "    sign_prod = torch.sign(true_diff) * torch.sign(pred_diff)\n",
    "    return torch.sum(torch.triu(sign_prod, diagonal=1)) / (n * (n - 1) / 2)\n",
    "\n",
    "# Class\n",
    "def f1_torch(y_true, y_logits, threshold=0.0):\n",
    "    y_pred = (y_logits > threshold).float()\n",
    "    f1 = f1_score(\n",
    "        y_true.cpu().numpy().astype(int),\n",
    "        y_pred.cpu().numpy().astype(int)\n",
    "    )\n",
    "    return torch.tensor(f1, device=y_true.device)\n",
    "\n",
    "def auroc_torch(y_true, y_logits):\n",
    "    auc = roc_auc_score(\n",
    "        y_true.cpu().numpy().astype(int),\n",
    "        y_logits.cpu().numpy()\n",
    "    )\n",
    "    return torch.tensor(auc, device=y_true.device)\n",
    "\n",
    "def auprc_torch(y_true, y_logits):\n",
    "    ap = average_precision_score(\n",
    "        y_true.cpu().numpy().astype(int),\n",
    "        y_logits.cpu().numpy()\n",
    "    )\n",
    "    return torch.tensor(ap, device=y_true.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75759b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EMBEDDING: ESM2_T33 =====\n",
      "\n",
      "--- Fold: LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1 ---\n",
      "Ep001 TrainL:0.8253 ValL:0.9150\n",
      "Ep050 TrainL:0.6755 ValL:1.2531\n",
      "Early stopping at epoch 81\n",
      "\n",
      "--- Fold: LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 ---\n",
      "Ep001 TrainL:0.8462 ValL:0.9346\n",
      "Ep050 TrainL:0.6729 ValL:1.1200\n",
      "Early stopping at epoch 81\n",
      "\n",
      "===== EMBEDDING: ESM2_T6 =====\n",
      "\n",
      "--- Fold: LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1 ---\n",
      "Ep001 TrainL:0.8636 ValL:1.0149\n",
      "Ep050 TrainL:0.6765 ValL:1.4950\n",
      "Early stopping at epoch 81\n",
      "\n",
      "--- Fold: LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 ---\n",
      "Ep001 TrainL:0.8483 ValL:0.9630\n",
      "Ep050 TrainL:0.6747 ValL:1.2428\n",
      "Early stopping at epoch 81\n",
      "\n",
      "===== EMBEDDING: ESMC =====\n",
      "\n",
      "--- Fold: LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1 ---\n",
      "Ep001 TrainL:0.8193 ValL:0.9110\n",
      "Ep050 TrainL:0.6715 ValL:1.3749\n",
      "Early stopping at epoch 81\n",
      "\n",
      "--- Fold: LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 ---\n",
      "Ep001 TrainL:0.8130 ValL:0.9673\n",
      "Ep050 TrainL:0.6724 ValL:1.0749\n",
      "Early stopping at epoch 82\n",
      "                                                        auprc            \\\n",
      "split                                                    test     train   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.564020  0.953830   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.501565  0.972253   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.601405  0.960984   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.748766  0.970378   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.486969  0.978082   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.508732  0.967363   \n",
      "\n",
      "                                                                  auroc  \\\n",
      "split                                                     val      test   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.592639  0.279167   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.786769  0.314286   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.570125  0.391667   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.770871  0.635714   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.495150  0.233333   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.731562  0.296429   \n",
      "\n",
      "                                                                         \\\n",
      "split                                                   train       val   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.791756  0.478571   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.829032  0.629167   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.808961  0.435714   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.839875  0.687500   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.864875  0.335714   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.815233  0.487500   \n",
      "\n",
      "                                                           f1            \\\n",
      "split                                                    test     train   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.410256  0.877193   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.090909  0.877193   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.769231  0.936842   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.740741  0.896947   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.769231  0.877193   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.000000  0.879377   \n",
      "\n",
      "                                                                kendall  ...  \\\n",
      "split                                                     val      test  ...   \n",
      "embedding fold                                                           ...   \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.240000 -0.312500  ...   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.533333 -0.149733  ...   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.640000 -0.114919  ...   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.769231  0.000000  ...   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.565217 -0.300403  ...   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.457143 -0.210339  ...   \n",
      "\n",
      "                                                                    mae  \\\n",
      "split                                                     val      test   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 -0.067736  1.503670   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.143145  1.529623   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.014260  1.630321   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.102823  1.843763   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 -0.192513  1.733681   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.062500  1.435452   \n",
      "\n",
      "                                                                         \\\n",
      "split                                                   train       val   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.925050  1.549429   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.921498  1.463860   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.978063  1.642606   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.943693  1.725546   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.988149  1.632893   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.925759  1.462626   \n",
      "\n",
      "                                                          mse            \\\n",
      "split                                                    test     train   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  2.800835  1.317599   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  2.857377  1.296595   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  3.206682  1.495650   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  3.935073  1.406038   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  3.573405  1.429193   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  2.564495  1.350374   \n",
      "\n",
      "                                                                     r2  \\\n",
      "split                                                     val      test   \n",
      "embedding fold                                                            \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  2.917958 -4.012977   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  2.634892 -4.407200   \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  3.230626 -4.739368   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  3.519871 -6.446593   \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  3.204445 -5.395734   \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  2.639265 -3.852961   \n",
      "\n",
      "                                                                         \n",
      "split                                                   train       val  \n",
      "embedding fold                                                           \n",
      "ESM2_T33  LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.105204 -4.521840  \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.119468 -3.715969  \n",
      "ESM2_T6   LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2 -0.015712 -5.113523  \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.045144 -5.299919  \n",
      "ESMC      LOCO_intraC4_PhuRafOx_TEST_UmaRafOx_VAL_2  0.029420 -5.063978  \n",
      "          LOCO_intraC4_UmaRafOx_TEST_PhuRafOx_VAL_1  0.082947 -3.723796  \n",
      "\n",
      "[6 rows x 21 columns]\n",
      "  embedding  split  mse_mean   mse_std   r2_mean    r2_std  mae_mean  \\\n",
      "0  ESM2_T33   test  2.829106  0.039981 -4.210088  0.278758  1.516647   \n",
      "1  ESM2_T33  train  1.307097  0.014852  0.112336  0.010086  0.923274   \n",
      "2  ESM2_T33    val  2.776424  0.200158 -4.118905  0.569837  1.506645   \n",
      "3   ESM2_T6   test  3.570878  0.515050 -5.592981  1.207191  1.737042   \n",
      "4   ESM2_T6  train  1.450844  0.063365  0.014716  0.043032  0.960878   \n",
      "5   ESM2_T6    val  3.375249  0.204527 -5.206721  0.131802  1.684076   \n",
      "6      ESMC   test  3.068950  0.713407 -4.624347  1.090906  1.584567   \n",
      "7      ESMC  train  1.389783  0.055734  0.056183  0.037849  0.956954   \n",
      "8      ESMC    val  2.921854  0.399643 -4.393887  0.947652  1.547760   \n",
      "\n",
      "    mae_std  kendall_mean  kendall_std   f1_mean    f1_std  auroc_mean  \\\n",
      "0  0.018351     -0.231116     0.115094  0.250583  0.225813    0.296726   \n",
      "1  0.002511      0.406124     0.108322  0.877193  0.000000    0.810394   \n",
      "2  0.060507      0.037704     0.149116  0.386667  0.207418    0.553869   \n",
      "3  0.150926     -0.057460     0.081260  0.754986  0.020145    0.513690   \n",
      "4  0.024303      0.369807     0.005660  0.916894  0.028210    0.824418   \n",
      "5  0.058648      0.058541     0.062623  0.704615  0.091380    0.561607   \n",
      "6  0.210880     -0.255371     0.063685  0.384615  0.543928    0.264881   \n",
      "7  0.044117      0.491000     0.033236  0.878285  0.001545    0.840054   \n",
      "8  0.120397     -0.065007     0.180322  0.511180  0.076420    0.411607   \n",
      "\n",
      "   auroc_std  auprc_mean  auprc_std  \n",
      "0   0.024833    0.532793   0.044162  \n",
      "1   0.026358    0.963041   0.013028  \n",
      "2   0.106487    0.689704   0.137271  \n",
      "3   0.172568    0.675085   0.104200  \n",
      "4   0.021859    0.965681   0.006642  \n",
      "5   0.178039    0.670498   0.141949  \n",
      "6   0.044615    0.497851   0.015389  \n",
      "7   0.035102    0.972722   0.007579  \n",
      "8   0.107329    0.613356   0.167169  \n"
     ]
    }
   ],
   "source": [
    "# ——— Hyperparam ———\n",
    "hidden_dim, shared_dim      = 250, 200\n",
    "lr, weight_decay            = 1e-4, 1e-5\n",
    "epochs, patience, batch_size = 400, 80, 64\n",
    "lam, threshold              = 1.0, -1.5\n",
    "val_frac                   = 0.125\n",
    "\n",
    "# ——— Data/Devuce ———\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resp_raw = torch.from_numpy(df['log10(Specific Activity)'].values).float().to(device)\n",
    "\n",
    "prot_emb33 = protein_embs[_C.T33](df[_C.protein].values)\n",
    "prot_emb6  = protein_embs[_C.T6] (df[_C.protein].values)\n",
    "prot_embc  = protein_embs[_C.trilo](df[_C.protein].values)\n",
    "mol_emb    = mol_embs['RDKit'](df[_C.mol].values)\n",
    "\n",
    "protein_arrays = {\n",
    "    _C.T33:   torch.from_numpy(prot_emb33).float().to(device),\n",
    "    _C.T6:    torch.from_numpy(prot_emb6) .float().to(device),\n",
    "    _C.trilo: torch.from_numpy(prot_embc) .float().to(device),\n",
    "}\n",
    "x_mol = torch.from_numpy(mol_emb).float().to(device)\n",
    "\n",
    "# ——— Result containers ———\n",
    "all_results    = []\n",
    "actual_indices = {}\n",
    "\n",
    "# ——— Embedding per fold index loop ———\n",
    "for emb_key, xprot in protein_arrays.items():\n",
    "    print(f\"\\n===== EMBEDDING: {emb_key} =====\")\n",
    "    for fold_name, split in ((k, s) for k, s in splits.items() if getattr(s, 'val', None) is not None and len(s.val) > 0):\n",
    "        print(f\"\\n--- Fold: {fold_name} ---\")\n",
    "\n",
    "        # Index partition\n",
    "        trainval_idx = split.train\n",
    "        test_idx     = split.test\n",
    "        val_idx      = split.val if len(split.val)>0 else []\n",
    "\n",
    "        if len(val_idx)==0:\n",
    "            train_idx, val_idx = train_test_split(\n",
    "                trainval_idx, test_size=val_frac,\n",
    "                shuffle=True, random_state=int(fold_name.split('_')[1])\n",
    "            )\n",
    "        else:\n",
    "            train_idx = trainval_idx\n",
    "\n",
    "        actual_indices[(emb_key,fold_name,'train')] = train_idx\n",
    "        actual_indices[(emb_key,fold_name,'val'  )] = val_idx\n",
    "        actual_indices[(emb_key,fold_name,'test' )] = test_idx\n",
    "\n",
    "        # Scaling \"y\" to [-1,1]\n",
    "        mn, mx   = resp_raw[train_idx].min(), resp_raw[train_idx].max()\n",
    "        y_scaled = 2*(resp_raw - mn)/(mx - mn) - 1\n",
    "\n",
    "        # DataLoaders (4 tensors)\n",
    "        ds = {}\n",
    "        for split_name, idxs in [('train',train_idx),('val',val_idx),('test',test_idx)]:\n",
    "            ds[split_name] = TensorDataset(\n",
    "                xprot[idxs], x_mol[idxs],\n",
    "                y_scaled[idxs], resp_raw[idxs]\n",
    "            )\n",
    "        loaders = {s: DataLoader(ds[s], batch_size, shuffle=(s=='train'))\n",
    "                   for s in ds}\n",
    "\n",
    "        # Model and optimizer\n",
    "        model     = MultiTaskEnzymatch(xprot.shape[1], x_mol.shape[1],\n",
    "                                  hidden_dim, shared_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=lr, weight_decay=weight_decay)\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "        mse = nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_ctr  = 0\n",
    "\n",
    "        # — Training & validation loop —\n",
    "        for epoch in range(1, epochs+1):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for xb_p, xb_m, yb_scaled, yb_raw in loaders['train']:\n",
    "                optimizer.zero_grad()\n",
    "                sims = model(xb_p, xb_m)\n",
    "                # Heads\n",
    "                pred_reg     = ((sims + 1)/2)*(mx-mn) + mn  # Rescaling to raw\n",
    "                logits_class = sims  # using sims as logit\n",
    "                yb_class     = (yb_raw > threshold).float()\n",
    "                # Loss\n",
    "                loss_cl = bce(logits_class, yb_class)\n",
    "                mask    = yb_class.bool()\n",
    "                loss_rg = mse(sims[mask], yb_scaled[mask]) if mask.any() else torch.tensor(0., device=device)\n",
    "                loss    = loss_cl + lam*loss_rg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * xb_p.size(0)\n",
    "            train_loss /= len(loaders['train'].dataset)\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb_p, xb_m, yv_scaled, yv_raw in loaders['val']:\n",
    "                    sims = model(xb_p, xb_m)\n",
    "                    yv_class = (yv_raw > threshold).float()\n",
    "                    l_cl = bce(sims, yv_class)\n",
    "                    m_v  = yv_class.bool()\n",
    "                    l_rg = mse(sims[m_v], yv_scaled[m_v]) if m_v.any() else torch.tensor(0., device=device)\n",
    "                    val_loss += (l_cl + lam*l_rg).item() * xb_p.size(0)\n",
    "            val_loss /= len(loaders['val'].dataset)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_ctr  = 0\n",
    "                torch.save(model.state_dict(),\n",
    "                           _C.results_path/'enzymatch_results'/'weights per split'/f'best_{emb_key}_{fold_name}.pt')\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "                if patience_ctr >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            if epoch==1 or epoch%50==0:\n",
    "                print(f\"Ep{epoch:03d} TrainL:{train_loss:.4f} ValL:{val_loss:.4f}\")\n",
    "\n",
    "        # — test —\n",
    "        model.load_state_dict(torch.load(\n",
    "            _C.results_path/'enzymatch_results'/'weights per split'/f'best_{emb_key}_{fold_name}.pt',\n",
    "            map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        # Saving predictions and results\n",
    "        for split_name, loader in loaders.items():\n",
    "            all_pr, all_tr = [], []\n",
    "            cls_logits, cls_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb_p, xb_m, ys_scaled, ys_raw in loader:\n",
    "                    sims = model(xb_p, xb_m)\n",
    "                    pr_raw = ((sims + 1)/2)*(mx-mn) + mn\n",
    "                    all_pr.append(pr_raw.cpu())\n",
    "                    all_tr.append(ys_raw.cpu())\n",
    "                    cls_logits.append(sims.cpu())\n",
    "                    cls_true.append((ys_raw>threshold).float().cpu())\n",
    "            pr = torch.cat(all_pr).numpy()\n",
    "            tr = torch.cat(all_tr).numpy()\n",
    "            lg = torch.cat(cls_logits).numpy()\n",
    "            yt = torch.cat(cls_true).numpy().astype(int)\n",
    "\n",
    "            # regression metrics\n",
    "            mse_v = ((tr-pr)**2).mean()\n",
    "            ssr   = ((tr-pr)**2).sum()\n",
    "            sst   = ((tr-tr.mean())**2).sum()\n",
    "            r2_v  = 1 - ssr/sst\n",
    "            mae_v = abs(tr-pr).mean()\n",
    "            kt_v  = kendall_tau(torch.from_numpy(tr).to(device),\n",
    "                                torch.from_numpy(pr).to(device)).item()\n",
    "\n",
    "            # classif metrics\n",
    "            f1_v   = f1_score(yt, (lg>0).astype(int))\n",
    "            auc_v  = roc_auc_score(yt, lg)\n",
    "            ap_v   = average_precision_score(yt, lg)\n",
    "\n",
    "            all_results.append({\n",
    "                'embedding': emb_key,\n",
    "                'fold':      fold_name,\n",
    "                'split':     split_name,\n",
    "                'mse':       mse_v,\n",
    "                'r2':        r2_v,\n",
    "                'mae':       mae_v,\n",
    "                'kendall':   kt_v,\n",
    "                'f1':        f1_v,\n",
    "                'auroc':     auc_v,\n",
    "                'auprc':     ap_v\n",
    "            })\n",
    "\n",
    "# ——— tidying ———\n",
    "df = pd.DataFrame(all_results)\n",
    "print(df.pivot_table(\n",
    "    index=['embedding','fold'],\n",
    "    columns='split',\n",
    "    values=['mse','r2','mae','kendall','f1','auroc','auprc'],\n",
    "    aggfunc='first'\n",
    "))\n",
    "\n",
    "summary = df.groupby(['embedding','split']).agg(\n",
    "    mse_mean     = ('mse','mean'),\n",
    "    mse_std      = ('mse','std'),\n",
    "    r2_mean      = ('r2','mean'),\n",
    "    r2_std       = ('r2','std'),\n",
    "    mae_mean     = ('mae','mean'),\n",
    "    mae_std      = ('mae','std'),\n",
    "    kendall_mean = ('kendall','mean'),\n",
    "    kendall_std  = ('kendall','std'),\n",
    "    f1_mean      = ('f1','mean'),\n",
    "    f1_std       = ('f1','std'),\n",
    "    auroc_mean   = ('auroc','mean'),\n",
    "    auroc_std    = ('auroc','std'),\n",
    "    auprc_mean   = ('auprc','mean'),\n",
    "    auprc_std    = ('auprc','std')\n",
    ").reset_index()\n",
    "\n",
    "print(summary)\n",
    "df.to_csv(_C.results_path/'enzymatch_results'/'by_fold.csv', index=False)\n",
    "summary.to_csv(_C.results_path/'enzymatch_results'/'summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "074d82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegressionEnzymatch(nn.Module):\n",
    "    \"\"\"\n",
    "    Predice un escalar en [-1, 1] a partir del coseno entre las proyecciones\n",
    "    de proteína y molécula. Úsala con y_scaled en [-1,1].\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_prot, dim_mol, hidden_dim=512, shared_dim=128, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.proj_p = nn.Sequential(\n",
    "            nn.Linear(dim_prot, hidden_dim), nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_dim, shared_dim)\n",
    "        )\n",
    "        self.proj_m = nn.Sequential(\n",
    "            nn.Linear(dim_mol, hidden_dim), nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_dim, shared_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_p, x_m):\n",
    "        zp = F.normalize(self.proj_p(x_p), dim=1)  # (B, d)\n",
    "        zm = F.normalize(self.proj_m(x_m), dim=1)  # (B, d)\n",
    "        yhat = (zp * zm).sum(dim=1)                # (B,) coseno ∈ [-1, 1]\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f2464ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'log10(Specific Activity)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'log10(Specific Activity)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ——— Data/Device ———\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m resp_raw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog10(Specific Activity)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m prot_emb33 \u001b[38;5;241m=\u001b[39m protein_embs[_C\u001b[38;5;241m.\u001b[39mT33](df[_C\u001b[38;5;241m.\u001b[39mprotein]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     12\u001b[0m prot_emb6  \u001b[38;5;241m=\u001b[39m protein_embs[_C\u001b[38;5;241m.\u001b[39mT6] (df[_C\u001b[38;5;241m.\u001b[39mprotein]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'log10(Specific Activity)'"
     ]
    }
   ],
   "source": [
    "# ——— Hyperparam ———\n",
    "hidden_dim, shared_dim       = 250, 200\n",
    "lr, weight_decay             = 1e-4, 1e-5\n",
    "epochs, patience, batch_size = 400, 80, 64\n",
    "val_frac                     = 0.125\n",
    "\n",
    "# ——— Data/Device ———\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resp_raw = torch.from_numpy(df['log10(Specific Activity)'].values).float().to(device)\n",
    "\n",
    "prot_emb33 = protein_embs[_C.T33](df[_C.protein].values)\n",
    "prot_emb6  = protein_embs[_C.T6] (df[_C.protein].values)\n",
    "prot_embc  = protein_embs[_C.trilo](df[_C.protein].values)\n",
    "mol_emb    = mol_embs['RDKit'](df[_C.mol].values)\n",
    "\n",
    "protein_arrays = {\n",
    "    _C.T33:   torch.from_numpy(prot_emb33).float().to(device),\n",
    "    _C.T6:    torch.from_numpy(prot_emb6) .float().to(device),\n",
    "    _C.trilo: torch.from_numpy(prot_embc) .float().to(device),\n",
    "}\n",
    "x_mol = torch.from_numpy(mol_emb).float().to(device)\n",
    "\n",
    "# ——— Result containers ———\n",
    "all_results    = []\n",
    "actual_indices = {}\n",
    "\n",
    "# ——— Embedding per fold index loop ———\n",
    "for emb_key, xprot in protein_arrays.items():\n",
    "    print(f\"\\n===== EMBEDDING: {emb_key} =====\")\n",
    "    # solo folds con validación\n",
    "    for fold_name, split in ((k, s) for k, s in splits.items() if getattr(s, 'val', None) is not None and len(s.val) > 0):\n",
    "        print(f\"\\n--- Fold: {fold_name} ---\")\n",
    "\n",
    "        # Index partition\n",
    "        trainval_idx = np.asarray(split.train, dtype=int)\n",
    "        test_idx     = np.asarray(split.test,  dtype=int)\n",
    "        val_idx      = np.asarray(split.val,   dtype=int)\n",
    "\n",
    "        if val_idx.size == 0:\n",
    "            # fallback (no debería ocurrir por el filtro de arriba)\n",
    "            train_idx, val_idx = train_test_split(\n",
    "                trainval_idx, test_size=val_frac,\n",
    "                shuffle=True, random_state=0\n",
    "            )\n",
    "        else:\n",
    "            train_idx = trainval_idx\n",
    "\n",
    "        actual_indices[(emb_key,fold_name,'train')] = train_idx\n",
    "        actual_indices[(emb_key,fold_name,'val'  )] = val_idx\n",
    "        actual_indices[(emb_key,fold_name,'test' )] = test_idx\n",
    "\n",
    "        # Scaling \"y\" to [-1,1] con protección numérica\n",
    "        mn, mx = resp_raw[train_idx].min(), resp_raw[train_idx].max()\n",
    "        den    = float(max((mx - mn).item() if torch.is_tensor(mx) else (mx - mn), 1e-8))\n",
    "        y_scaled = 2*(resp_raw - mn)/den - 1\n",
    "\n",
    "        # DataLoaders (4 tensores)\n",
    "        ds = {}\n",
    "        for split_name, idxs in [('train',train_idx),('val',val_idx),('test',test_idx)]:\n",
    "            ds[split_name] = TensorDataset(\n",
    "                xprot[idxs], x_mol[idxs],\n",
    "                y_scaled[idxs], resp_raw[idxs]\n",
    "            )\n",
    "        loaders = {s: DataLoader(ds[s], batch_size, shuffle=(s=='train')) for s in ds}\n",
    "\n",
    "        # Model and optimizer (solo regresión)\n",
    "        model     = RegressionEnzymatch(xprot.shape[1], x_mol.shape[1], hidden_dim, shared_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        mse       = nn.MSELoss()  # (si tu cola es pesada, prueba SmoothL1Loss(beta=0.2))\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_ctr  = 0\n",
    "\n",
    "        # — Training & validation loop —\n",
    "        for epoch in range(1, epochs+1):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for xb_p, xb_m, yb_scaled, _yb_raw in loaders['train']:\n",
    "                optimizer.zero_grad()\n",
    "                pred_scaled = model(xb_p, xb_m)              # (B,)\n",
    "                loss = mse(pred_scaled, yb_scaled)           # solo regresión en [-1,1]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * xb_p.size(0)\n",
    "            train_loss /= len(loaders['train'].dataset)\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb_p, xb_m, yv_scaled, _yv_raw in loaders['val']:\n",
    "                    pred_scaled = model(xb_p, xb_m)\n",
    "                    val_loss += mse(pred_scaled, yv_scaled).item() * xb_p.size(0)\n",
    "            val_loss /= len(loaders['val'].dataset)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_ctr  = 0\n",
    "                torch.save(model.state_dict(),\n",
    "                           _C.results_path/'enzymatch_results'/'weights per split'/f'best_{emb_key}_{fold_name}.pt')\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "                if patience_ctr >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            if epoch==1 or epoch%50==0:\n",
    "                print(f\"Ep{epoch:03d} TrainL:{train_loss:.4f} ValL:{val_loss:.4f}\")\n",
    "\n",
    "        # — test —\n",
    "        model.load_state_dict(torch.load(\n",
    "            _C.results_path/'enzymatch_results'/'weights per split'/f'best_{emb_key}_{fold_name}.pt',\n",
    "            map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        # Saving predictions and results\n",
    "        for split_name, loader in loaders.items():\n",
    "            all_pr_raw, all_tr_raw = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb_p, xb_m, _ys_scaled, ys_raw in loader:\n",
    "                    pred_scaled = model(xb_p, xb_m)\n",
    "                    pr_raw = ((pred_scaled + 1)/2)*den + mn      # back to raw\n",
    "                    all_pr_raw.append(pr_raw.cpu())\n",
    "                    all_tr_raw.append(ys_raw.cpu())\n",
    "\n",
    "            pr = torch.cat(all_pr_raw).numpy()\n",
    "            tr = torch.cat(all_tr_raw).numpy()\n",
    "\n",
    "            # regression metrics\n",
    "            mse_v = ((tr-pr)**2).mean()\n",
    "            ssr   = ((tr-pr)**2).sum()\n",
    "            sst   = ((tr-tr.mean())**2).sum()\n",
    "            r2_v  = 1 - ssr/sst if sst > 0 else 0.0\n",
    "            mae_v = np.abs(tr-pr).mean()\n",
    "            kt_v  = kendall_tau(torch.from_numpy(tr).to(device),\n",
    "                                torch.from_numpy(pr).to(device)).item()\n",
    "\n",
    "            all_results.append({\n",
    "                'embedding': emb_key,\n",
    "                'fold':      fold_name,\n",
    "                'split':     split_name,\n",
    "                'mse':       mse_v,\n",
    "                'r2':        r2_v,\n",
    "                'mae':       mae_v,\n",
    "                'kendall':   kt_v,\n",
    "            })\n",
    "\n",
    "# ——— tidying ———\n",
    "df_res = pd.DataFrame(all_results)\n",
    "print(df_res.pivot_table(\n",
    "    index=['embedding','fold'],\n",
    "    columns='split',\n",
    "    values=['mse','r2','mae','kendall'],\n",
    "    aggfunc='first'\n",
    "))\n",
    "\n",
    "summary = df_res.groupby(['embedding','split']).agg(\n",
    "    mse_mean     = ('mse','mean'),\n",
    "    mse_std      = ('mse','std'),\n",
    "    r2_mean      = ('r2','mean'),\n",
    "    r2_std       = ('r2','std'),\n",
    "    mae_mean     = ('mae','mean'),\n",
    "    mae_std      = ('mae','std'),\n",
    "    kendall_mean = ('kendall','mean'),\n",
    "    kendall_std  = ('kendall','std')\n",
    ").reset_index()\n",
    "\n",
    "print(summary)\n",
    "df_res.to_csv(_C.results_path/'enzymatch_results'/'by_fold.csv', index=False)\n",
    "summary.to_csv(_C.results_path/'enzymatch_results'/'summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "202c82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for (fold_name, emb_key, split_name), y_true_array in true_dict.items():\n",
    "    y_pred_array = preds_dict[(fold_name, emb_key, split_name)] #array predictions\n",
    "    #Retrieve index list for this dataframe per fold and per split:\n",
    "    idxs = actual_indices[(fold_name, split_name)]\n",
    "    \n",
    "    #WAlk trough each example in this set:\n",
    "    for i, idx in enumerate(idxs):\n",
    "        rows.append({\n",
    "            'fold':      fold_name,                 # e.g. \"Random_1\"\n",
    "            'embedding': emb_key,                   # e.g. \"ESM2_T33\"\n",
    "            'split':     split_name,                # \"train\" / \"val\" / \"test\"\n",
    "            'protein':   df.loc[idx, _C.protein],\n",
    "            'mol':       df.loc[idx, _C.mol],\n",
    "            'y_true':    float(y_true_array[i]),\n",
    "            'y_pred':    float(y_pred_array[i])\n",
    "        })\n",
    "\n",
    "# Turn to dataframe and save it\n",
    "clip_preds = pd.DataFrame(rows)\n",
    "clip_preds.to_csv(_C.results_path / 'CLIP results' / 'clip_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Chemistry Wild Ventures)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
